{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca84e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "\n",
      "Sample reviews from the dataset:\n",
      "Review 1 (Sentiment: Positive):\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert is an amazing actor and now the same b...\n",
      "Review 2 (Sentiment: Negative):\n",
      "big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst e...\n",
      "Review 3 (Sentiment: Negative):\n",
      "this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching...\n",
      "Review 4 (Sentiment: Positive):\n",
      "the at storytelling the traditional sort many years after the event i can still see in my eye an elderly lady my friend's mother retelling the battle of she makes the characters come alive her passion...\n",
      "Review 5 (Sentiment: Negative):\n",
      "worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of ...\n",
      "\n",
      "Training dataset size: 25000 reviews\n",
      "Testing dataset size: 25000 reviews\n",
      "\n",
      "Class distribution in training data:\n",
      "Negative: 12500 (50.0%)\n",
      "Positive: 12500 (50.0%)\n",
      "\n",
      "Preprocessing data...\n",
      "Training data shape after padding: (25000, 200)\n",
      "Testing data shape after padding: (25000, 200)\n",
      "\n",
      "Building LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prithviraj Ghorpade\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 1s/step - accuracy: 0.6560 - loss: 0.5984 - val_accuracy: 0.8282 - val_loss: 0.4220\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 784ms/step - accuracy: 0.8594 - loss: 0.3535 - val_accuracy: 0.8534 - val_loss: 0.3509\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 811ms/step - accuracy: 0.9112 - loss: 0.2434 - val_accuracy: 0.8514 - val_loss: 0.3940\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 763ms/step - accuracy: 0.9038 - loss: 0.2575 - val_accuracy: 0.8468 - val_loss: 0.4535\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 1s/step - accuracy: 0.9325 - loss: 0.1888 - val_accuracy: 0.8248 - val_loss: 0.4338\n",
      "\n",
      "Evaluating the model...\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 95ms/step - accuracy: 0.8433 - loss: 0.3764\n",
      "Test Loss: 0.3788\n",
      "Test Accuracy: 0.8402\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 89ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.83      0.84     12500\n",
      "    Positive       0.84      0.85      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n",
      "\n",
      "Predictions on sample reviews:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Review: This movie was fantastic! The acting was superb an...\n",
      "Sentiment: Positive (Score: 0.6289)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Review: Terrible film. Waste of time and money. The plot m...\n",
      "Sentiment: Negative (Score: 0.0431)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Review: It was an okay movie. Not great, not terrible, jus...\n",
      "Sentiment: Negative (Score: 0.2186)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Review: I was pleasantly surprised by this film. The revie...\n",
      "Sentiment: Positive (Score: 0.9590)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print('Downloading NLTK stopwords...')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# a) Select and load a suitable dataset\n",
    "print('\\nLoading dataset...')\n",
    "\n",
    "# Load the IMDB Movie Reviews dataset from Keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load the data with the top 10,000 words\n",
    "max_words = 10000\n",
    "(X_train_raw, y_train), (X_test_raw, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "# Get the word index mapping\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Reverse the word index to get words\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "# Function to convert sequences back to text\n",
    "def seq_to_text(sequence):\n",
    "    # Index offset is 3 because 0 = padding, 1 = start, 2 = unknown\n",
    "    return ' '.join([reverse_word_index.get(i-3, '?') for i in sequence if i > 3])\n",
    "\n",
    "# Convert some examples back to text for inspection\n",
    "train_reviews = [seq_to_text(sequence) for sequence in X_train_raw[:5]]\n",
    "print('\\nSample reviews from the dataset:')\n",
    "for i, review in enumerate(train_reviews):\n",
    "    print(f\"Review {i+1} (Sentiment: {'Positive' if y_train[i] == 1 else 'Negative'}):\")\n",
    "    print(review[:200] + \"...\") # Print just the beginning of the review\n",
    "\n",
    "# Display dataset information\n",
    "print(f'\\nTraining dataset size: {len(X_train_raw)} reviews')\n",
    "print(f\"Testing dataset size: {len(X_test_raw)} reviews\")\n",
    "\n",
    "# Display class distribution\n",
    "train_sentiment_counts = np.bincount(y_train)\n",
    "test_sentiment_counts = np.bincount(y_test)\n",
    "\n",
    "print('\\nClass distribution in training data:')\n",
    "print(f\"Negative: {train_sentiment_counts[0]} ({train_sentiment_counts[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Positive: {train_sentiment_counts[1]} ({train_sentiment_counts[1]/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y_train)\n",
    "plt.title('Sentiment Distribution (Training Data)')\n",
    "plt.xlabel('Sentiment (0: Negative, 1: Positive)')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_test)\n",
    "plt.title('Sentiment Distribution (Test Data)')\n",
    "plt.xlabel('Sentiment (0: Negative, 1: Positive)')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sentiment_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Data preprocessing\n",
    "print(\"\\nPreprocessing data...\")\n",
    "\n",
    "# Define the max sequence length\n",
    "max_len = 200 # Maximum sequence length\n",
    "\n",
    "# Pad the sequences\n",
    "X_train = pad_sequences(X_train_raw, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test_raw, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Training data shape after padding: {X_train.shape}\")\n",
    "print(f\"Testing data shape after padding: {X_test.shape}\")\n",
    "\n",
    "# b) Apply RNN variant (LSTM) for prediction\n",
    "print(\"\\nBuilding LSTM model...\")\n",
    "\n",
    "# Define the model\n",
    "def create_lstm_model(vocab_size, embedding_dim=128, lstm_units=64):\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "        \n",
    "        # Bidirectional LSTM layer\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        Bidirectional(LSTM(lstm_units)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and display the model\n",
    "vocab_size = max_words + 1 # +1 for the padding token\n",
    "model = create_lstm_model(vocab_size)\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Evaluate the model\n",
    "print('\\nEvaluating the model...')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Display classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Function to predict sentiment on new texts\n",
    "def predict_sentiment_raw(text, model, max_len=200):\n",
    "    # Get the word index\n",
    "    word_index = imdb.get_word_index()\n",
    "    \n",
    "    # Preprocess the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = []\n",
    "    for word in words:\n",
    "        # Add 3 because 0 = padding, 1 = start, 2 = unknown\n",
    "        idx = word_index.get(word, 0) + 3\n",
    "        if idx < max_words + 3: # Only include words in our vocabulary\n",
    "            sequence.append(idx)\n",
    "    \n",
    "    # Pad the sequence\n",
    "    padded = pad_sequences([sequence], maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded)[0][0]\n",
    "    \n",
    "    return {\n",
    "        'review': text,\n",
    "        'sentiment_score': float(prediction),\n",
    "        'sentiment': 'Positive' if prediction > 0.5 else 'Negative'\n",
    "    }\n",
    "\n",
    "# Test the model with a few sample reviews\n",
    "sample_reviews = [\n",
    "    \"This movie was fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
    "    \"Terrible film. Waste of time and money. The plot made no sense and the acting was wooden.\",\n",
    "    \"It was an okay movie. Not great, not terrible, just average entertainment.\",\n",
    "    \"I was pleasantly surprised by this film. The reviews weren't great but I really enjoyed it!\"\n",
    "]\n",
    "\n",
    "print('\\nPredictions on sample reviews:')\n",
    "for review in sample_reviews:\n",
    "    result = predict_sentiment_raw(review, model)\n",
    "    print(f\"Review: {review[:50]}...\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (Score: {result['sentiment_score']:.4f})\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
